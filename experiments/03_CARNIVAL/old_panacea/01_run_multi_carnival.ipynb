{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57338fc-3dc3-462d-90b0-e56cc49a29d5",
   "metadata": {},
   "source": [
    "# Multi CARNIVAL on PANACEA\n",
    "\n",
    "This notebook shows how the results can be generated. To generate the results for the manuscript, we used the `script.py` in a HPC for convenience. Multi-condition methods require a high performance MILP solver. We used GUROBI for all experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39495b2b-8436-422b-ad74-57acab992e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corneto as cn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "selected_drug = \"PONATINIB\"\n",
    "selected_cells = [\"H1793\", \"LNCAP\", \"KRJ1\", \"HCC1143\", \"EFO21\", \"PANC1\", \"HF2597\"]\n",
    "\n",
    "df_panacea = pd.read_csv(\"GSE186341-PANACEA.tsv.xz\", sep='\\t')\n",
    "df_panacea['drug'] = df_panacea['obs_id'].str.extract(r'_(.*?)_v')\n",
    "df_panacea['cell'] = df_panacea['obs_id'].str.extract( r'^([^_]*)')\n",
    "df_panacea['sign'] = np.sign(df_panacea['act'])\n",
    "\n",
    "def filter_df(\n",
    "    cells,\n",
    "    drugs,\n",
    "    df = df_panacea, \n",
    "    resource = \"dorothea\", \n",
    "    pipeline = \"NA+deseq2\", \n",
    "    statparam = \"stat\", \n",
    "    status = \"unfiltered\", \n",
    "    padj = 0.05\n",
    "):\n",
    "    c = [c.upper() for c in cells]\n",
    "    d = [d.upper() for d in drugs]\n",
    "    dff = df[\n",
    "        (df.cell.str.upper().isin(c)) & \n",
    "        (df.drug.str.upper().isin(d)) & \n",
    "        (df.resource == resource) & \n",
    "        (df.pipeline == pipeline) &\n",
    "        (df.statparam == statparam) &\n",
    "        (df.status == status) &\n",
    "        (df.padj <= padj)\n",
    "    ]\n",
    "    return dff\n",
    "\n",
    "df_conditions = filter_df(selected_cells, [selected_drug])\n",
    "\n",
    "def get_measurements(cell, drug, df = df_panacea, resource = \"dorothea\", pipeline = \"NA+deseq2\", statparam = \"stat\", padj = 0.05, as_dict=True):\n",
    "    df_r = df[\n",
    "        (df.drug.str.upper() == drug.upper()) & \n",
    "        (df.cell.str.upper() == cell.upper()) & \n",
    "        (df.resource == resource) & \n",
    "        (df.pipeline == pipeline) &\n",
    "        (df.statparam == statparam) &\n",
    "        (df.padj <= padj)\n",
    "    ]\n",
    "    if as_dict:\n",
    "        return df_r[[\"items\", \"sign\"]].set_index(\"items\").to_dict()[\"sign\"]\n",
    "    return df_r\n",
    "\n",
    "G_pkn = cn.Graph.from_sif(\"network_collectri.sif\", has_header=True, column_order=[0, 2, 1])\n",
    "ponatinib_targets = [\"BCR\", \"ABL\", \"VEGFR\", \"PDGFRA_PDGFRB\", \"FGFR1\", \"FGFR2\", \"FGFR3\", \"FGFR4\", \"EPH\", \"SRC\", \"KIT\", \"RET\", \"TIE2\", \"FLT3\"]\n",
    "targets = {v: -1 for v in G_pkn.V if v in ponatinib_targets}\n",
    "\n",
    "# Create the input dict\n",
    "input_data = dict()\n",
    "for cell in selected_cells:\n",
    "    d = dict()\n",
    "    input_data[cell] = d\n",
    "    d[\"input\"] = targets\n",
    "    d[\"output\"] = get_measurements(cell, selected_drug)\n",
    "\n",
    "\n",
    "from corneto.methods.future.carnival import Carnival\n",
    "from corneto.methods.future.method import Dataset\n",
    "from corneto.methods.signalling.carnival import multi_carnival\n",
    "import pandas as pd\n",
    "\n",
    "n_reps = 10\n",
    "timelimit=120\n",
    "norel=120\n",
    "gt_lambda = 0.1\n",
    "\n",
    "sols = []\n",
    "for i in range(n_reps):\n",
    "    P, Gexp, stats = multi_carnival(G_pkn, input_data, lambd=gt_lambda)\n",
    "    P.solve(solver=\"GUROBI\", TimeLimit=timelimit, NoRelHeurTime=norel, Seed=i, verbosity=0);\n",
    "    sols.append(P.expr.edge_value.value)\n",
    "    print(i, \"completed\")\n",
    "\n",
    "\n",
    "# Ground truth data. A script should be used to generate and save a dataset (ground truth)\n",
    "# passing the arguments n_reps, timelimit, norel and gt_lambda\n",
    "df_gt = pd.DataFrame(np.mean(sols, axis=0), index=Gexp.E)\n",
    "\n",
    "\n",
    "### Measuring performance\n",
    "\n",
    "import random\n",
    "\n",
    "def split_inputs_outputs_folds(data, folds, seed=None):\n",
    "    \"\"\"\n",
    "    Splits the data into a list of (train, test) pairs of dictionaries, \n",
    "    one pair per fold. For each key in `data`, its 'output' entries are randomly \n",
    "    partitioned into `folds` groups. In fold i:\n",
    "      - The test set (test_dict) contains the outputs from group i.\n",
    "      - The train set (train_dict) contains all the remaining outputs.\n",
    "    Over all folds, every output is used exactly once as test data.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The original dictionary with 'input' and 'output' keys.\n",
    "        folds (int): Number of folds (train/test splits) to generate.\n",
    "        seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples (train_dict, test_dict), one tuple per fold.\n",
    "              Each train_dict contains for every key:\n",
    "                  {'input': <original input>, 'output': <training outputs>}\n",
    "              Each test_dict contains for every key:\n",
    "                  {'output': <test outputs>}\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # Initialize a list of (train, test) dictionary pairs, one for each fold.\n",
    "    folds_list = []\n",
    "    for _ in range(folds):\n",
    "        folds_list.append(({}, {}))  # Each element is a tuple: (train_dict, test_dict)\n",
    "\n",
    "    # Process each key in the data.\n",
    "    for key, value in data.items():\n",
    "        input_features = value['input']\n",
    "        outputs = value['output']\n",
    "        output_keys = list(outputs.keys())\n",
    "        total_outputs = len(output_keys)\n",
    "        \n",
    "        # Shuffle the list of output keys.\n",
    "        random.shuffle(output_keys)\n",
    "        \n",
    "        # Compute the size for each fold so that the outputs are as evenly distributed as possible.\n",
    "        fold_sizes = [total_outputs // folds] * folds\n",
    "        remainder = total_outputs % folds\n",
    "        for i in range(remainder):\n",
    "            fold_sizes[i] += 1\n",
    "        \n",
    "        # Partition the output_keys into `folds` parts.\n",
    "        assigned = []\n",
    "        start = 0\n",
    "        for size in fold_sizes:\n",
    "            assigned.append(output_keys[start:start + size])\n",
    "            start += size\n",
    "\n",
    "        # For each fold, build the train and test dictionaries for this key.\n",
    "        for i, (train_dict, test_dict) in enumerate(folds_list):\n",
    "            # In fold i, the test set gets the outputs in assigned[i],\n",
    "            # and the training set gets all other outputs.\n",
    "            test_keys = assigned[i]\n",
    "            # The union of the remaining parts are the training outputs.\n",
    "            # (Since output_keys is the entire set, we simply remove the test_keys.)\n",
    "            train_keys = [k for k in output_keys if k not in test_keys]\n",
    "            \n",
    "            # Add to the training dictionary: include the input and the training outputs.\n",
    "            train_dict[key] = {\n",
    "                'input': input_features,\n",
    "                'output': {k: outputs[k] for k in train_keys}\n",
    "            }\n",
    "            # In the test dictionary, only include the outputs.\n",
    "            test_dict[key] = {\n",
    "                'output': {k: outputs[k] for k in test_keys}\n",
    "            }\n",
    "\n",
    "    return folds_list\n",
    "\n",
    "num_folds = 5\n",
    "seed = 42  # For reproducibility\n",
    "\n",
    "folds_data = split_inputs_outputs_folds(input_data, num_folds, seed)\n",
    "folds_data[0][0]\n",
    "\n",
    "lambdas = [0, 0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30]\n",
    "\n",
    "errors = dict()\n",
    "for l in lambdas:\n",
    "    fold_errors = []\n",
    "    for fold in folds_data:\n",
    "        data = fold[0]\n",
    "        sols_rec = []\n",
    "        for i in range(n_reps):\n",
    "            P, Gexp, stats = multi_carnival(G_pkn, data, lambd=l)\n",
    "            P.solve(solver=\"GUROBI\", TimeLimit=timelimit, NoRelHeurTime=norel, Seed=i, verbosity=0);\n",
    "            sols_rec.append(P.expr.edge_value.value)\n",
    "            print(i, \"completed\")\n",
    "        err = (df_gt - pd.DataFrame(np.mean(sols_rec, axis=0), index=Gexp.E)).fillna(1).pow(2).mean(axis=0).sum()\n",
    "        fold_errors.append(err)\n",
    "    errors[l] = fold_errors\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6b666-29f7-4bf9-9c8e-0e63c0196ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
